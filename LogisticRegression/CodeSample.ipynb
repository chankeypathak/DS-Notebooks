{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Sample: LogisticRegression, Grid and Random Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use breast_cancer data set from sklearn.datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Data\n",
    "\n",
    "Classes\t2<br>\n",
    "Samples per class\t212(M),357(B)<br>\n",
    "Samples total\t569<br>\n",
    "Dimensionality\t30<br>\n",
    "Features\treal, positive<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About LogisticRegression\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0, warm_start=False, n_jobs=1)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Explanation of parameters:<br>\n",
    "\n",
    "penalty -> is Regularization Term 'L1' or 'L2' Regularization<br>\n",
    "\n",
    "C -> 1/lambda -> lambda is hyper parameter (check notebook for explanation)<br>\n",
    "\n",
    "fit_intercept -> Whether to have the intercept term or not i.e our eq is W.T*X without intercept i.e pass through origin, with intercept W.T*X+b<br>\n",
    "\n",
    "class_weight => When data is imbalanced it performs UpSampling or Down Sampling i.e if DTrain has 10% positive points and 90% negative points then we can give class weights as [9,1] so that data will be balanced its upsampling <br> \n",
    "\n",
    "solver => Which algo to be used in case of optimization problem<br>\n",
    "\n",
    "max_iter -> no.of iterations, The way Optimization problem solves the LR is iterative way.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)<br>\n",
    "\n",
    "#### Explanation of parameters<br>\n",
    "\n",
    "estimator => is ntg but what is the function on which we are going to perform GridSearch, in our case its LR<br>\n",
    "\n",
    "param_grid => If we have one Hyper Parameter(lambda) then we can give the values of that parameter in the form of dictionary, if we have 2 params we need to give in terms of list of distionaries.<br>\n",
    "\n",
    "scoring => accuracy, f1 score etc...<br>\n",
    "\n",
    "n_jobs => multi core processing\n",
    "\n",
    "pre_dispatch => no.of jobs get dispatched for parallel execution.\n",
    "its val should be small(recommended) to avoid memory overflow<br>\n",
    "\n",
    "cv => by defauld it performs 3 fold cv, if mention number then it performs that many folds cv<br>\n",
    "\n",
    "\n",
    "refit => if we found best lambda at the end it will refit the data<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, random_state=None, error_score=’raise’, return_train_score=’warn’)<br>\n",
    "\n",
    "##### Every parameter is same as GridSearchCv except second parameter.\n",
    "<br>\n",
    "param_distribution=>scipy.stats.distributions, we need to use this type of distributions i.e val can be anything from this distribution<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Hyper param(lambda) vals\n",
    "# here 'C' is ntg but 1/(lambda)\n",
    "tuned_parameters = [{'C':[10**-4,10**-2,10**0,10**2,10**4]}]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data,data.target,train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Score:  0.9647058823529412\n"
     ]
    }
   ],
   "source": [
    "#using GridSearchCV\n",
    "model = GridSearchCV(LogisticRegression(),tuned_parameters,\n",
    "                     scoring='f1',cv=5)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print(model.best_estimator_)\n",
    "print(\"Score: \",model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking How Sparsity Wors with L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4402271 ,  0.        ,  0.26669648, -0.00345869,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.05038257,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.46141917, -0.13497397, -0.13210411, -0.02055487,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "clf = LogisticRegression(C=0.1, penalty='l1');\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "w = clf.coef_\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i.e when c=0.1 i.e lambda= 1/0.1 =10 with L1 Regularization we got 8 non zero values => these 8 features are important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.14808882,  0.00792713,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.01887019,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -0.02188808,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "clf = LogisticRegression(C=0.01, penalty='l1');\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "w = clf.coef_\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i.e when c=0.01 i.e lambda= 1/0.01 =100 with L1 Regularization we got 4 non zero values => these 4 features are important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: as C val is decreasing or Lambda val is increasing the Sparsity increases, if we increase more Sparsity then model will be underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
